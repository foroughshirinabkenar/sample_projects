{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1fd6a0d-70fa-4444-92b3-ea6fea873f09",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this file, we aim to categorize the data into four groups, namely 'Normal (no failure)', 'Failure_1 (failure in link one)', 'Failure_2 (failure in link two)', and 'Failure_3 (failure in link three)'. To this end, we use dataset 888_voa_3 (supervised) as the source dataset and all other datasets in the form of unsupervised. The objective is to develop a DANN (Domain Adversarial Neural Network) model to classify link failures in different datasets. We first implement the k-means clustring on each unsupervised dataset to generate psudo-labels relying on which we develop the DANN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4f20d4-4120-4852-afba-e76ef29e333e",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dadec27-16f1-4e26-9684-67f527f0a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6a095-5701-4299-82b0-fbdeec7cb6bc",
   "metadata": {},
   "source": [
    "## Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1d8539-d6f9-48fe-9ac0-3be3c6c44549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc75147d-5bfa-4bd1-9917-3a0bbd63c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b23ecc50-bb25-4fb0-b9d4-185d6b64cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(X, y, batch_size=64, shuffle=True):\n",
    "    input_dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long))\n",
    "    data_loader = DataLoader(input_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efe52e-4191-4c4b-b69b-9cc7b764fef6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d0d7aec-98b0-4623-bb54-371cbc175cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "    @staticmethod\n",
    "    def grad_reverse(x, alpha=1.0):\n",
    "        return GradReverse.apply(x, alpha)\n",
    "\n",
    "class DANN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=4, drop_prob=0.3):\n",
    "        super(DANN, self).__init__()\n",
    "        hidden_dim_last = hidden_dim // 2\n",
    "        hidden_dim_cd = input_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim_last),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_last, hidden_dim_last),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_last, hidden_dim_cd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.class_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim_cd, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "        \n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim_cd, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = \"mps\"\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "    \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x, alpha=1.0):\n",
    "        features = self.feature_extractor(x)\n",
    "        reverse_features = GradReverse.grad_reverse(features, alpha)\n",
    "        class_output = self.class_classifier(features)\n",
    "        domain_output = self.domain_classifier(reverse_features)\n",
    "    \n",
    "        return class_output, domain_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe35e25-5b93-4508-b633-0c99607706d5",
   "metadata": {},
   "source": [
    "## Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba8a346f-7987-4378-ab4e-62ecf25028e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_s, y_s, X_t, y_t, X_e, y_e):    \n",
    "    warnings.filterwarnings('ignore')\n",
    "    clone_gpu()\n",
    "    set_seed(seed=42)\n",
    "\n",
    "    num_epochs = 1000\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "    weight = 0.1\n",
    "    ep_print = num_epochs // 10\n",
    "\n",
    "    X_src = copy.deepcopy(X_s)\n",
    "    y_src = copy.deepcopy(y_s)\n",
    "    X_tgt = copy.deepcopy(X_t)\n",
    "    y_tgt = copy.deepcopy(y_t)\n",
    "    X_val = copy.deepcopy(X_e)\n",
    "    y_val = copy.deepcopy(y_e)\n",
    "\n",
    "    dataloader_source = prepare_train_test_data(X_src, y_src, batch_size=batch_size)\n",
    "    dataloader_target = prepare_train_test_data(X_tgt, y_tgt, batch_size=batch_size)\n",
    "\n",
    "    dict_unq_src = get_unique_label_data(X_src, y_src)\n",
    "    dict_unq_tgt = get_unique_label_data(X_tgt, y_tgt)\n",
    "\n",
    "    dataloader_src = []\n",
    "    dataloader_tgt = []\n",
    "    for key in dict_unq_src.keys():\n",
    "        X_source = dict_unq_src[key]\n",
    "        y_source = torch.ones(len(X_source), dtype=torch.long)*key\n",
    "        dl_s = prepare_train_test_data(X_source, y_source, batch_size=batch_size)\n",
    "        dataloader_src.append(dl_s)\n",
    "\n",
    "        X_target = dict_unq_tgt[key]\n",
    "        y_target = torch.ones(len(X_target), dtype=torch.long)*key\n",
    "        dl_t = prepare_train_test_data(X_target, y_target, batch_size=batch_size)\n",
    "        dataloader_tgt.append(dl_t)\n",
    "\n",
    "    dataloader_val = prepare_train_test_data(X_val, y_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = X_src.shape[1]\n",
    "    hidden_dim = 100\n",
    "    output_dim = 4\n",
    "\n",
    "    model = DANN(input_dim, hidden_dim)\n",
    "    \n",
    "    criterion_class = nn.NLLLoss().cuda()\n",
    "    criterion_domain = nn.NLLLoss().cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    unique_labels = [i for i in range(output_dim)]\n",
    "\n",
    "    loss_history = []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        for dtl_src, dtl_tgt in zip(dataloader_src, dataloader_tgt):\n",
    "            len_dataloader = min(len(dtl_src), len(dtl_tgt))\n",
    "            data_zip = enumerate(zip(dtl_src, dtl_tgt))\n",
    "    \n",
    "            model.train()\n",
    "            loss_list = []\n",
    "            total_loss = 0\n",
    "            for step, ((source_data, source_label), (target_data, _)) in data_zip:\n",
    "                rho = float(step + epoch * len_dataloader) / (num_epochs * len_dataloader)\n",
    "                alpha = 2. / (1. + np.exp(-10 * rho)) - 1\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                source_data = source_data.cuda()\n",
    "                source_label = source_label.cuda()\n",
    "                target_data = target_data.cuda()\n",
    "\n",
    "                # source data forward\n",
    "                domain_label = torch.zeros(len(source_data), dtype=torch.long).cuda()\n",
    "                class_output, domain_output = model(source_data, alpha)\n",
    "                loss_class_source = criterion_class(class_output, source_label)\n",
    "                loss_domain_source = criterion_domain(domain_output, domain_label)\n",
    "    \n",
    "                # target data forward\n",
    "                domain_label = torch.ones(len(target_data), dtype=torch.long).cuda()\n",
    "                _, domain_output = model(target_data, alpha)\n",
    "                loss_domain_target = criterion_domain(domain_output, domain_label)\n",
    "    \n",
    "                # loss\n",
    "                loss_class = loss_class_source\n",
    "                loss_domain = loss_domain_source + loss_domain_target\n",
    "                loss = loss_class + loss_domain\n",
    "                total_loss += loss\n",
    "                \n",
    "            loss = total_loss / step\n",
    "            loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        accuracy = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader_val:\n",
    "                inputs = inputs.cuda()\n",
    "                predictions, _ = model(inputs)\n",
    "                _, y_pred = torch.max(predictions.data, 1)\n",
    "                acc = accuracy_score(y_pred.cpu(), labels)\n",
    "                accuracy.append(acc * 100)\n",
    "                \n",
    "        accuracy_mean = np.mean(accuracy)\n",
    "        mean_loss = np.mean(loss_list)\n",
    "        loss_history.append(np.round(mean_loss, 2))\n",
    "        if (epoch + 1) % ep_print == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy_mean: .2f}\")\n",
    "            \n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    return runtime, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965eb9a-fbb6-43db-ada5-b54797704d63",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04b18a47-f78c-40ce-b472-d4443c2ed6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluations(model, dt, end_ind, start_ind, batch_size=64):\n",
    "    acc_cat = np.zeros((4, 4), dtype=np.float32)\n",
    "    num_samples_acc = np.zeros(4, dtype=np.float32)\n",
    "    acc = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_val, y_val = get_data(dict_datasets[dt], end_ind, start_ind)\n",
    "        val_loader = prepare_train_test_data(X_val, y_val, batch_size=batch_size, shuffle=False)\n",
    "        for i, (inputs, labels) in enumerate(val_loader):    \n",
    "            inputs = inputs.cuda()\n",
    "            \n",
    "            predictions, _ = model(inputs)\n",
    "            _, y_pred = torch.max(predictions.data, 1)\n",
    "            y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "            acc.append(accuracy_score(y_pred, labels) * 100)\n",
    "\n",
    "            for lbl_gt, lbl_pred in zip(labels, y_pred):\n",
    "                acc_cat[lbl_gt][lbl_pred] += 1\n",
    "                num_samples_acc[lbl_gt] += 1\n",
    "        \n",
    "    accuracy = np.mean(acc)\n",
    "    for i in range(acc_cat.shape[0]):\n",
    "        for j in range(acc_cat.shape[1]):\n",
    "            acc_cat[i][j] = acc_cat[i][j] * 100 / num_samples_acc[i]\n",
    "\n",
    "    return_list = [accuracy]\n",
    "    for acc in acc_cat:\n",
    "        return_list.append(acc)\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a765a-d55b-4fa6-8187-d7ceb900ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "X_src, y_src = get_data(dict_datasets[src_dataset], train_size)\n",
    "models = []\n",
    "\n",
    "for i, dt in enumerate(eval_total):\n",
    "    print('Dataset:', dt)\n",
    "    X_tgt_i, y_tgt_i = get_data(dict_datasets[dt], train_size)\n",
    "    X_val_i, y_val_i = get_data(dict_datasets[dt], train_size + 100, train_size)\n",
    "    runtime, model = train(X_src, y_src, X_tgt_i, y_tgt_i, X_val_i, y_val_i)\n",
    "    res = evaluations(model, dt, val_size, train_size)\n",
    "    models.append(model)\n",
    "\n",
    "    new_row = [f'{dt}']\n",
    "    for r in res:\n",
    "        new_row.append(r)\n",
    "    new_row.append(runtime)\n",
    "\n",
    "    results.append(new_row)\n",
    "    print(f'Accuracy: {res[0]}, Runitme: {runtime}')\n",
    "    print('*****************************************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
